# core/live_analyzer.py

"""
å³æ™‚åˆ†æå¼•æ“ (LiveAnalyzer) - VLM æ•´åˆç‰ˆ
åŠŸèƒ½ï¼š
1. DeepFace æœ¬åœ°æƒ…ç·’ (è¡Œç‚ºæ¨¡å¼)ã€‚
2. GPT-4o VLM é¤ç›¤æ´å¯Ÿ (é¤ç›¤æ¨¡å¼)ã€‚
3. é›™æ¨¡å¼åˆ†æµèˆ‡å¿«å–æ©Ÿåˆ¶ã€‚
4. â˜… [NEW] å½±åƒä½è­‰å„²å­˜èˆ‡ DB é€£çµã€‚
"""

import cv2
import time
import threading
import asyncio
from queue import Queue, Empty, Full
import numpy as np
import os 
from datetime import datetime
import platform # ç”¨æ–¼åˆ¤æ–·ä½œæ¥­ç³»çµ±

# â˜…â˜…â˜… å„ªåŒ– 1ï¼šå°‡ DeepFace ç§»è‡³æœ€ä¸Šæ–¹è¼‰å…¥ï¼Œé¿å…åŸ·è¡Œç·’å…§é‡è¤‡è¼‰å…¥é€ æˆå¡é “ â˜…â˜…â˜…
try:
    from deepface import DeepFace
    print("âœ… DeepFace æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError:
    DeepFace = None
    print("âš ï¸ DeepFace æ¨¡çµ„æœªå®‰è£ï¼Œæƒ…ç·’åˆ†æå°‡ç„¡æ³•ä½¿ç”¨ (è«‹åŸ·è¡Œ pip install deepface tf-keras)")

from services.database import insert_log 

from services.vision_analysis import (
    HeadGestureDetector,
    crop_face_with_mediapipe,
    estimate_plate_leftover,
    detect_food_regions_yolo, 
)
from services import llm_handler as llm 

import config as _cfg
from .types import AnalysisResult

EMOTE_INTERVAL_SECONDS   = getattr(_cfg, "EMOTE_INTERVAL_SECONDS", 2.0)
CAMERA_RESOLUTION_WIDTH  = getattr(_cfg, "CAMERA_RESOLUTION_WIDTH", 1280)
CAMERA_RESOLUTION_HEIGHT = getattr(_cfg, "CAMERA_RESOLUTION_HEIGHT", 720)
CAMERA_BUFFER_SIZE       = getattr(_cfg, "CAMERA_BUFFER_SIZE", 1)
CAMERA_INDEX             = getattr(_cfg, "CAMERA_INDEX", 0)

VLM_INTERVAL_SECONDS = 10.0
LOG_INTERVAL_SECONDS = 5.0 
EVIDENCE_DIR = "session_evidence" 

class LiveAnalyzer:
    def __init__(self, model_pack: dict, menu_items: list, analysis_options: dict, db_manager):
        self.model_pack = model_pack
        self.menu_items = menu_items
        self.analysis_options = analysis_options
        
        self._frame_display_queue = Queue(maxsize=1)
        self._frame_analysis_queue = Queue(maxsize=1)
        self._analysis_result_queue = Queue(maxsize=1)

        self.gesture_detector = HeadGestureDetector()

        self._stop_event = threading.Event()
        self._camera_thread = None
        self._worker_thread = None
        
        self.db_manager = db_manager
        self.session_id = datetime.now().strftime("%Y%m%d%H%M%S") 
        os.makedirs(EVIDENCE_DIR, exist_ok=True) 
        
        # æƒ…ç·’åˆ†æç‹€æ…‹
        self._llm_busy = False 
        self._last_emote_ts = 0.0
        self._cached_emotion = "ä¸­æ€§"      
        self._new_emotion_arrived = False 

        # VLM é¤ç›¤åˆ†æç‹€æ…‹
        self._vlm_busy = False
        self._last_vlm_ts = 0.0
        self._cached_plate_insight = None 
        self._new_insight_arrived = False
        
        self._last_log_ts = 0.0
        self._cached_token_usage = None
        self._cached_food_detections = [] 

        # Latch é–å®šæ©Ÿåˆ¶
        self._latched_nod = False
        self._latched_shake = False
        self._latched_emotion = None
        self._latch_lock = threading.Lock()

    # -------------------------------------------------
    #  åŸ·è¡Œç·’ 1ï¼šæ”å½±æ©Ÿ
    # -------------------------------------------------
    def _camera_loop(self):
        system_os = platform.system()
        print(f"ğŸ“· æ­£åœ¨å•Ÿå‹•æ”å½±æ©Ÿ... (åµæ¸¬ç³»çµ±: {system_os})")

        cap = None
        # 1. æ ¹æ“šç³»çµ±é¸æ“‡é–‹å•Ÿæ–¹å¼
        if system_os == "Darwin": # macOS
             cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_AVFOUNDATION)
        elif system_os == "Windows": # Windows
             # Windows å„ªå…ˆä½¿ç”¨ DSHOW
             cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
        
        # 2. å¦‚æœå¤±æ•—ï¼Œé€€å›é è¨­
        if cap is None or not cap.isOpened():
            print("âš ï¸ å°ˆå±¬æ¨¡å¼é–‹å•Ÿå¤±æ•—ï¼Œå˜—è©¦é è¨­æ¨¡å¼...")
            cap = cv2.VideoCapture(CAMERA_INDEX)

        if cap is None or not cap.isOpened():
            print("âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ (è«‹æª¢æŸ¥é€£æ¥æˆ–æ˜¯è¢«å…¶ä»–ç¨‹å¼ä½”ç”¨)")
            return

        cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_RESOLUTION_WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_RESOLUTION_HEIGHT)

        while not self._stop_event.is_set():
            ok, frame = cap.read()
            if not ok:
                time.sleep(0.01)
                continue

            # æ”¾å…¥é¡¯ç¤ºä½‡åˆ—
            if self._frame_display_queue.full():
                try: self._frame_display_queue.get_nowait()
                except Empty: pass
            self._frame_display_queue.put_nowait(frame)

            # æ”¾å…¥åˆ†æä½‡åˆ—
            if self._frame_analysis_queue.full():
                try: self._frame_analysis_queue.get_nowait()
                except Empty: pass
            self._frame_analysis_queue.put_nowait(frame)
            
            time.sleep(0.005) 
            
        cap.release()
        print("ğŸ“· æ”å½±æ©Ÿå·²é‡‹æ”¾")

    def _save_evidence(self, event_type, frame, frame_count):
        try:
            filename = f"{self.session_id}_{event_type}_{frame_count}.jpg"
            path = os.path.join(EVIDENCE_DIR, filename)
            if not os.path.exists(EVIDENCE_DIR):
                os.makedirs(EVIDENCE_DIR)
            cv2.imwrite(path, frame)
            self.db_manager.save_event_evidence(
                session_id=self.session_id, 
                event_type=event_type, 
                local_path=path
            )
        except Exception as e:
            print(f"Evidence Save Error: {e}")

    # -------------------------------------------------
    #  åŸ·è¡Œç·’ 2ï¼šCV åˆ†æ
    # -------------------------------------------------
    def _analysis_worker(self):
        client = self.model_pack.get("client")
        pose_detector = self.model_pack.get("pose_detector")
        face_detector = self.model_pack.get("face_detector")
        detector = self.gesture_detector
        frame_count = 0
        
        cached_plate_label = None
        cached_plate_ratio = None 
        cached_plate_circle = None
        cached_food_dets = [] 

        last_debug_print_ts = 0 

        while not self._stop_event.is_set():
            try:
                frame = self._frame_analysis_queue.get(timeout=0.5) # ç¸®çŸ­ timeout
            except Empty:
                continue

            result = AnalysisResult()
            frame_count += 1

            # äººæ•¸è¨ˆç®—
            current_people_count = 0
            if face_detector:
                try:
                    small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                    rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                    face_results = face_detector.process(rgb_frame)
                    if face_results.detections:
                        current_people_count = len(face_results.detections)
                except Exception: pass
            
            result.display_info["people_count"] = current_people_count

            # (A) é»é ­/æ–é ­
            if self.analysis_options.get("opt_nod") and pose_detector:
                try:
                    small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                    rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                    res = pose_detector.process(rgb)
                    if res.pose_landmarks:
                        lm = res.pose_landmarks.landmark
                        dx = lm[0].x - (lm[7].x + lm[8].x + lm[11].x + lm[12].x) / 4.0
                        dy = lm[0].y - (lm[7].y + lm[8].y + lm[11].y + lm[12].y) / 4.0
                        event = detector.update_and_classify(dx, dy)
                        
                        if event == "nod":
                            with self._latch_lock: self._latched_nod = True
                            self._save_evidence("nod", frame.copy(), frame_count)
                        elif event == "shake":
                            with self._latch_lock: self._latched_shake = True
                            self._save_evidence("shake", frame.copy(), frame_count)
                except Exception: pass

            # (B) é¤ç›¤åµæ¸¬
            if self.analysis_options.get("opt_plate"):
                if frame_count % 15 == 0:
                    try:
                        label, ratio, circle = estimate_plate_leftover(frame)
                        if label in ["å‰©é¤˜50%ä»¥ä¸Š", "ç„¡å‰©é¤˜"]:
                            cached_plate_label = label
                            cached_plate_ratio = ratio 
                        else:
                            cached_plate_label = None 
                            cached_plate_ratio = None
                        cached_plate_circle = circle
                        cached_food_dets = detect_food_regions_yolo(frame)
                        self._cached_food_detections = cached_food_dets
                    except Exception: pass
                
                if cached_plate_label:
                    result.plate_event = cached_plate_label 
                    display_text = f"{cached_plate_label} ({cached_plate_ratio:.0%})" if cached_plate_ratio else cached_plate_label
                    result.display_info["plate_label"] = display_text

                if cached_plate_circle: result.display_info["plate_circle"] = cached_plate_circle
                result.display_info["food_detections"] = cached_food_dets

                # VLM è§¸ç™¼
                now = time.time()
                should_trigger = (cached_plate_label is not None or len(cached_food_dets) > 0)
                is_cooldown = (now - self._last_vlm_ts) < VLM_INTERVAL_SECONDS
                
                if should_trigger and (now - last_debug_print_ts > 3.0):
                    if not client: print("âš ï¸ [VLM Warning] æœªè¨­å®š OpenAI API Key")
                    elif self._vlm_busy: print("â³ [VLM Skip] ç³»çµ±å¿™ç¢Œä¸­")
                    last_debug_print_ts = now

                if should_trigger and client and not self._vlm_busy and not is_cooldown:
                    self._vlm_busy = True 
                    self._last_vlm_ts = now 
                    print(f"ğŸš€ VLM è§¸ç™¼æˆåŠŸ!")
                    self._save_evidence("plate_vlm", frame.copy(), frame_count)
                    threading.Thread(target=self._run_vlm_background, 
                                     args=(frame.copy(), client, cached_food_dets)).start()

            # (C) DeepFace è¡¨æƒ… (ä½¿ç”¨å…¨åŸŸ DeepFace)
            now = time.time()
            if (self.analysis_options.get("opt_emote") and 
                DeepFace is not None and  # ç¢ºä¿æ¨¡çµ„æœ‰è¼‰å…¥
                not self._llm_busy and (now - self._last_emote_ts) > EMOTE_INTERVAL_SECONDS):
                
                self._llm_busy = True
                self._last_emote_ts = now
                threading.Thread(target=self._run_deepface_background, 
                                 args=(frame.copy(), face_detector)).start()

            if self._cached_plate_insight: result.plate_insight = self._cached_plate_insight
            if self._cached_token_usage:
                result.token_usage_event = self._cached_token_usage
                self._cached_token_usage = None 

            # è‡ªå‹•è¨˜éŒ„ Log
            now = time.time()
            if (now - self._last_log_ts) > LOG_INTERVAL_SECONDS: 
                if current_people_count > 0 or cached_plate_label:
                    emotions_data = {self._cached_emotion: 1.0} if self._cached_emotion else {}
                    try:
                        insert_log(
                            source_type="live_stream",
                            people_count=current_people_count,
                            emotions=emotions_data,
                            food_detected=cached_plate_label if cached_plate_label else "ç„¡"
                        )
                        self._last_log_ts = now
                    except Exception: pass
            
            # æ¨é€çµæœ
            if self._analysis_result_queue.full():
                try: self._analysis_result_queue.get_nowait()
                except Empty: pass
            self._analysis_result_queue.put_nowait(result)
            
            # â˜…â˜…â˜… å„ªåŒ– 2ï¼šå¾®å°å»¶é²è®“å‡º CPUï¼Œè§£æ±ºç•«é¢å¡é “ â˜…â˜…â˜…
            time.sleep(0.005)

    def _run_vlm_background(self, frame, client, food_detections):
        try:
            async def task():
                insight, usage = await llm.analyze_plate_vlm(frame, client) 
                return insight, usage
            insight, usage = asyncio.run(task())
            
            if insight:
                self._cached_plate_insight = insight 
                self._new_insight_arrived = True 
            if usage:
                current = self._cached_token_usage or {"total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}
                current["total_tokens"] += usage.total_tokens
                current["prompt_tokens"] += usage.prompt_tokens
                current["completion_tokens"] += usage.completion_tokens
                self._cached_token_usage = current
        except Exception as e:
            print(f"VLM Error: {e}")
        finally:
            self._vlm_busy = False 

    def _run_deepface_background(self, frame, face_detector):
        # é€™è£¡ä¸éœ€å† import DeepFaceï¼Œç›´æ¥ä½¿ç”¨å…¨åŸŸè®Šæ•¸
        try:
            face_crop = crop_face_with_mediapipe(frame, face_detector)
            if face_crop is None: return

            analysis = DeepFace.analyze(
                img_path=face_crop, 
                actions=['emotion'], 
                enforce_detection=False, 
                detector_backend='skip', 
                silent=True
            )
            dominant = analysis[0]['dominant_emotion']
            mapping = {
                "happy": "é–‹å¿ƒ", "neutral": "å¹³æ·¡", "sad": "å¤±æœ›", 
                "angry": "ä¸æ»¿", "surprise": "é©šè‰·", "fear": "å›°æƒ‘", "disgust": "å«Œæ£„"
            }
            final_emotion = mapping.get(dominant, dominant)

            # åŒæ­¥æ›´æ–°å¿«å– (çµ¦ DB/åœ–è¡¨) å’Œ é–å®š (çµ¦ UI æ—¥èªŒ)
            self._cached_emotion = final_emotion 
            with self._latch_lock:
                self._latched_emotion = final_emotion 
                
            print(f"âœ… æƒ…ç·’åµæ¸¬: {final_emotion}")

        except Exception as e:
            print(f"DeepFace Error: {e}")
        finally:
            self._llm_busy = False

    def start(self):
        if self._camera_thread and self._camera_thread.is_alive(): return
        self._stop_event.clear()
        self._camera_thread = threading.Thread(target=self._camera_loop, daemon=True)
        self._worker_thread = threading.Thread(target=self._analysis_worker, daemon=True)
        self._camera_thread.start()
        self._worker_thread.start()

    def stop(self):
        self._stop_event.set()
        time.sleep(0.5)
        self._camera_thread = None
        self._worker_thread = None

    def get_latest_frame(self):
        try: return self._frame_display_queue.get_nowait()
        except Empty: return None

    @property
    def raw_session_id(self): return self.session_id

    def get_latest_analysis_result(self):
        try:
            result = self._analysis_result_queue.get_nowait()
        except Empty:
            return None

        with self._latch_lock:
            if self._latched_nod:
                result.nod_event = True
                self._latched_nod = False
            if self._latched_shake:
                result.shake_event = True 
                self._latched_shake = False
            
            # å–å‡ºæƒ…ç·’äº‹ä»¶
            if self._latched_emotion:
                result.emotion_event = self._latched_emotion
                self._latched_emotion = None 

        return result