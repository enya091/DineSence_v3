# core/live_analyzer.py

"""
å³æ™‚åˆ†æå¼•æ“ (LiveAnalyzer) - VLM æ•´åˆç‰ˆ
åŠŸèƒ½ï¼š
1. DeepFace æœ¬åœ°æƒ…ç·’ (è¡Œç‚ºæ¨¡å¼)ã€‚
2. GPT-4o VLM é¤ç›¤æ´å¯Ÿ (é¤ç›¤æ¨¡å¼)ã€‚
3. é›™æ¨¡å¼åˆ†æµèˆ‡å¿«å–æ©Ÿåˆ¶ã€‚
4. â˜… [NEW] å½±åƒä½è­‰å„²å­˜èˆ‡ DB é€£çµã€‚
"""

import cv2
import time
import threading
import asyncio
from queue import Queue, Empty, Full
import numpy as np
import os 
from datetime import datetime
import platform # ç”¨æ–¼åˆ¤æ–·ä½œæ¥­ç³»çµ±

# â˜…â˜…â˜… å„ªåŒ– 1ï¼šå°‡ DeepFace ç§»è‡³æœ€ä¸Šæ–¹è¼‰å…¥ï¼Œé¿å…åŸ·è¡Œç·’å…§é‡è¤‡è¼‰å…¥é€ æˆå¡é “ â˜…â˜…â˜…
try:
    from deepface import DeepFace
    print("âœ… DeepFace æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError:
    DeepFace = None
    print("âš ï¸ DeepFace æ¨¡çµ„æœªå®‰è£ï¼Œæƒ…ç·’åˆ†æå°‡ç„¡æ³•ä½¿ç”¨ (è«‹åŸ·è¡Œ pip install deepface tf-keras)")

from services.database import insert_log 

from services.vision_analysis import (
    HeadGestureDetector,
    crop_face_with_mediapipe,
    estimate_plate_leftover,
    detect_food_regions_yolo, 
)
from services import llm_handler as llm 

import config as _cfg
from .types import AnalysisResult

from services.llm_handler import (
    analyze_plate_vlm, 
    identify_food_item  # â˜… [NEW] è¨˜å¾—å¼•å…¥é€™å€‹ï¼
)

EMOTE_INTERVAL_SECONDS   = getattr(_cfg, "EMOTE_INTERVAL_SECONDS", 2.0)
CAMERA_RESOLUTION_WIDTH  = getattr(_cfg, "CAMERA_RESOLUTION_WIDTH", 1280)
CAMERA_RESOLUTION_HEIGHT = getattr(_cfg, "CAMERA_RESOLUTION_HEIGHT", 720)
CAMERA_BUFFER_SIZE       = getattr(_cfg, "CAMERA_BUFFER_SIZE", 1)
FACE_CAM_INDEX  = getattr(_cfg, "FACE_CAM_INDEX", 1)  # é è¨­ 0 = Mac ç­†é›»é¡é ­
PLATE_CAM_INDEX = getattr(_cfg, "PLATE_CAM_INDEX", 0)  # é è¨­ 1 = æ‰‹æ©Ÿé¡é ­ï¼ˆCamo ç­‰ï¼‰


VLM_INTERVAL_SECONDS = 10.0
LOG_INTERVAL_SECONDS = 5.0 
EVIDENCE_DIR = "session_evidence" 

class LiveAnalyzer:
    def __init__(self, model_pack: dict, menu_items: list, analysis_options: dict, db_manager):
        self.model_pack = model_pack
        self.menu_items = menu_items
        self.analysis_options = analysis_options
        
        self._face_display_queue = Queue(maxsize=1)
        self._face_analysis_queue = Queue(maxsize=1)
        
        self._plate_display_queue = Queue(maxsize=1)
        self._plate_analysis_queue = Queue(maxsize=1)
        
        # çµæœä½‡åˆ—ç¶­æŒä¸€å€‹ï¼Œå› ç‚ºæˆ‘å€‘è¦åˆä½µçµæœå‚³çµ¦ UI
        self._analysis_result_queue = Queue(maxsize=1)

        self.gesture_detector = HeadGestureDetector()

        self._stop_event = threading.Event()
        
        # [ä¿®æ”¹] æº–å‚™å…©å€‹ç›¸æ©ŸåŸ·è¡Œç·’
        self._face_cam_thread = None
        self._plate_cam_thread = None
        self._worker_thread = None
        
        self.db_manager = db_manager
        self.session_id = datetime.now().strftime("%Y%m%d%H%M%S") 
        os.makedirs(EVIDENCE_DIR, exist_ok=True) 
        
        # æƒ…ç·’åˆ†æç‹€æ…‹
        self._llm_busy = False 
        self._last_emote_ts = 0.0
        self._cached_emotion = "ä¸­æ€§"      
        self._new_emotion_arrived = False 

        # VLM é¤ç›¤åˆ†æç‹€æ…‹
        self._vlm_busy = False
        self._last_vlm_ts = 0.0
        self._cached_plate_insight = None 
        self._new_insight_arrived = False
        
        self._last_log_ts = 0.0
        self._cached_token_usage = None
        self._cached_food_detections = [] 

        # Latch é–å®šæ©Ÿåˆ¶
        self._latched_nod = False
        self._latched_shake = False
        self._latched_emotion = None
        self._latch_lock = threading.Lock()

        # äººè‡‰ç›¸é—œç‹€æ…‹
        self._current_people_count = 0
        
        # é¤ç›¤ç›¸é—œç‹€æ…‹
        self._cached_plate_label = None
        self._cached_plate_ratio = None
        self._cached_plate_circle = None
        self._cached_food_detections = []
        
        # è¼”åŠ©è®Šæ•¸
        self._frame_count = 0
        self._last_debug_print_ts = 0
        self._cross_capture_signal = None
        

    # -------------------------------------------------
    #  åŸ·è¡Œç·’ 1ï¼šæ”å½±æ©Ÿ
    # -------------------------------------------------
    def _open_camera(self, index, width, height):
        system_os = platform.system()
        print(f"ğŸ“· æ­£åœ¨é–‹å•Ÿé¡é ­ ID {index} (ç³»çµ±: {system_os})...")
        cap = None
        
        if system_os == "Darwin": # macOS
             cap = cv2.VideoCapture(index, cv2.CAP_AVFOUNDATION)
        elif system_os == "Windows": # Windows
             cap = cv2.VideoCapture(index, cv2.CAP_DSHOW)
        
        if cap is None or not cap.isOpened():
            cap = cv2.VideoCapture(index) # å¤±æ•—é€€å›é è¨­

        if cap.isOpened():
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
            
        return cap

    # [ä¿®æ”¹] äººè‡‰é¡é ­è¿´åœˆ (ç­†é›»é¡é ­)
    def _face_cam_loop(self):
        print(f"[DEBUG] Face camera using index = {FACE_CAM_INDEX}")
        # å‡è¨­ 0 æ˜¯ç­†é›»é¡é ­ï¼Œè§£æåº¦ 1280x720
        cap = self._open_camera(FACE_CAM_INDEX, 1280, 720) 
        
        while not self._stop_event.is_set():
            ok, frame = cap.read()
            if not ok:
                time.sleep(0.1); continue

            # æ”¾å…¥ Face çš„ä½‡åˆ—
            if self._face_display_queue.full(): 
                try: self._face_display_queue.get_nowait()
                except Empty: pass
            self._face_display_queue.put_nowait(frame)

            if self._face_analysis_queue.full():
                try: self._face_analysis_queue.get_nowait()
                except Empty: pass
            self._face_analysis_queue.put_nowait(frame)
            
            time.sleep(0.005)
        if cap: cap.release()

    # [ä¿®æ”¹] é¤ç›¤é¡é ­è¿´åœˆ (å¤–æ¥é¡é ­)
    def _plate_cam_loop(self):
        print(f"[DEBUG] Plate camera using index = {PLATE_CAM_INDEX}")
        # å‡è¨­ 1 æ˜¯å¤–æ¥é¡é ­ï¼Œè§£æåº¦å¯ä»¥ç”¨é«˜ä¸€é»ä¾‹å¦‚ 1920x1080 çœ‹æ¸…æ¥šé£Ÿç‰©
        cap = self._open_camera(PLATE_CAM_INDEX, 1920, 1080)
        
        while not self._stop_event.is_set():
            ok, frame = cap.read()
            if not ok:
                time.sleep(0.1); continue

            # æ”¾å…¥ Plate çš„ä½‡åˆ—
            if self._plate_display_queue.full():
                try: self._plate_display_queue.get_nowait()
                except Empty: pass
            self._plate_display_queue.put_nowait(frame)

            if self._plate_analysis_queue.full():
                try: self._plate_analysis_queue.get_nowait()
                except Empty: pass
            self._plate_analysis_queue.put_nowait(frame)

            time.sleep(0.005)
        if cap: cap.release()

    def _save_evidence(self, event_type, frame, frame_count):
        try:
            filename = f"{self.session_id}_{event_type}_{frame_count}.jpg"
            path = os.path.join(EVIDENCE_DIR, filename)
            if not os.path.exists(EVIDENCE_DIR):
                os.makedirs(EVIDENCE_DIR)
            cv2.imwrite(path, frame)
            self.db_manager.save_event_evidence(
                session_id=self.session_id, 
                event_type=event_type, 
                local_path=path
            )
        except Exception as e:
            print(f"Evidence Save Error: {e}")

    def _process_face_task(self, frame, result):
        """è™•ç†äººè‡‰é¡é ­çš„é‚è¼¯ï¼šäººæ•¸ã€å‹•ä½œã€è¡¨æƒ…"""
        face_detector = self.model_pack.get("face_detector")
        pose_detector = self.model_pack.get("pose_detector")
        
        # (A) è¨ˆç®—äººæ•¸
        if face_detector:
            try:
                small_frame = cv2.resize(frame, (0, 0), fx=0.75, fy=0.75)
                rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                face_results = face_detector.process(rgb_frame)
                self._current_people_count = len(face_results.detections) if face_results.detections else 0
            except Exception: pass
        
        result.display_info["people_count"] = self._current_people_count

        # (B) é»é ­/æ–é ­åµæ¸¬
        if self.analysis_options.get("opt_nod") and pose_detector:
            try:
                small_frame = cv2.resize(frame, (0, 0), fx=0.75, fy=0.75)
                rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
                res = pose_detector.process(rgb)
                if res.pose_landmarks:
                    lm = res.pose_landmarks.landmark
                    dx = lm[0].x - (lm[7].x + lm[8].x + lm[11].x + lm[12].x) / 4.0
                    dy = lm[0].y - (lm[7].y + lm[8].y + lm[11].y + lm[12].y) / 4.0
                    
                    event = self.gesture_detector.update_and_classify(dx, dy)
                    
                    if event == "nod":
                        with self._latch_lock: self._latched_nod = True
                        self._save_evidence("nod", frame.copy(), self._frame_count)
                    elif event == "shake":
                        with self._latch_lock: self._latched_shake = True
                        self._save_evidence("shake", frame.copy(), self._frame_count)
            except Exception: pass

        # (C) æƒ…ç·’åµæ¸¬ (è§¸ç™¼èƒŒæ™¯åŸ·è¡Œç·’)
        now = time.time()
        if (self.analysis_options.get("opt_emote") and 
            DeepFace is not None and 
            not self._llm_busy and 
            (now - self._last_emote_ts) > EMOTE_INTERVAL_SECONDS):
            
            self._llm_busy = True
            self._last_emote_ts = now
            threading.Thread(target=self._run_deepface_background, 
                             args=(frame.copy(), face_detector)).start()
    # -------------------------------------------------

    def _process_plate_task(self, frame, result, client):
        """è™•ç†é¤ç›¤é¡é ­çš„é‚è¼¯ï¼šå‰©é£Ÿè¨ˆç®—ã€VLM è§¸ç™¼"""
        if not self.analysis_options.get("opt_plate"):
            return

        # (A) åŸºç¤æ¼”ç®—æ³• (æ¯ 15 å¹€æ›´æ–°ä¸€æ¬¡å¿«å–)
        if self._frame_count % 15 == 0:
            try:
                label, ratio, circle = estimate_plate_leftover(frame)
                if label in ["å‰©é¤˜50%ä»¥ä¸Š", "ç„¡å‰©é¤˜"]:
                    self._cached_plate_label = label
                    self._cached_plate_ratio = ratio 
                else:
                    self._cached_plate_label = None 
                    self._cached_plate_ratio = None
                self._cached_plate_circle = circle
                # self._cached_food_detections = detect_food_regions_yolo(frame)
            except Exception: pass
        
        # å¡«å…¥ Result
        if self._cached_plate_label:
            result.plate_event = self._cached_plate_label 
            display_text = f"{self._cached_plate_label} ({self._cached_plate_ratio:.0%})" \
                           if self._cached_plate_ratio else self._cached_plate_label
            result.display_info["plate_label"] = display_text

        if self._cached_plate_circle: 
            result.display_info["plate_circle"] = self._cached_plate_circle
        
        result.display_info["food_detections"] = self._cached_food_detections

        # (B) VLM è§¸ç™¼åˆ¤æ–·
        now = time.time()
        should_trigger = (self._cached_plate_label is not None or len(self._cached_food_detections) > 0)
        is_cooldown = (now - self._last_vlm_ts) < VLM_INTERVAL_SECONDS
        
        # Debug è¨Šæ¯
        if should_trigger and (now - self._last_debug_print_ts > 3.0):
            if not client: print("âš ï¸ [VLM Warning] æœªè¨­å®š OpenAI API Key")
            elif self._vlm_busy: print("â³ [VLM Skip] ç³»çµ±å¿™ç¢Œä¸­")
            self._last_debug_print_ts = now

        if should_trigger and client and not self._vlm_busy and not is_cooldown:
            self._vlm_busy = True 
            self._last_vlm_ts = now 
            print(f"ğŸš€ VLM è§¸ç™¼æˆåŠŸ!")
            self._save_evidence("plate_vlm", frame.copy(), self._frame_count)
            threading.Thread(target=self._run_vlm_background, 
                             args=(frame.copy(), client, self._cached_food_detections)).start()
            
    def _sync_log_task(self):
        """æª¢æŸ¥ä¸¦åŸ·è¡Œè³‡æ–™åŒæ­¥å„²å­˜"""
        now = time.time()
        if (now - self._last_log_ts) > LOG_INTERVAL_SECONDS: 
            # åªæœ‰ç•¶æœ‰äººæˆ–æœ‰é¤ç›¤ç‹€æ…‹æ™‚æ‰ç´€éŒ„
            if self._current_people_count > 0 or self._cached_plate_label:
                
                emotions_data = {self._cached_emotion: 1.0} if self._cached_emotion else {}
                food_data = self._cached_plate_label if self._cached_plate_label else "ç„¡"

                try:
                    insert_log(
                        source_type="live_dual_cam",
                        people_count=self._current_people_count,
                        emotions=emotions_data,
                        food_detected=food_data
                    )
                    self._last_log_ts = now
                except Exception as e: 
                    print(f"Log Error: {e}")

    # [æ–°å¢] è¼”åŠ©å‡½å¼ï¼šå­˜æª”ç”¨ (æ”¾åœ¨é¡åˆ¥å…§)
    def _save_custom_file(self, filename, frame):
        try:
            path = os.path.join(EVIDENCE_DIR, filename)
            cv2.imwrite(path, frame)
            return path
        except Exception: return None

    #  åŸ·è¡Œç·’ 2ï¼šCV åˆ†æ
    # -------------------------------------------------
    def _analysis_worker(self):
        client = self.model_pack.get("client")
        
        while not self._stop_event.is_set():
            # 1. ç²å–ç•«é¢
            face_frame = None
            plate_frame = None
            try: face_frame = self._face_analysis_queue.get_nowait()
            except Empty: pass
            try: plate_frame = self._plate_analysis_queue.get_nowait()
            except Empty: pass

            if face_frame is None and plate_frame is None:
                time.sleep(0.005); continue

            result = AnalysisResult() 
            self._frame_count += 1

            # 2. åŸ·è¡Œä»»å‹™ (æ¨¡çµ„åŒ–)
            if face_frame is not None:
                self._process_face_task(face_frame, result)
            
            if plate_frame is not None:
                self._process_plate_task(plate_frame, result, client)

            # =========================================================
            # â˜…â˜…â˜… [æ–°å¢] è™•ç†å¼·çƒˆæƒ…ç·’çš„ã€Œé›™é¡é ­é€£æ‹ã€ â˜…â˜…â˜…
            # =========================================================
            if self._cross_capture_signal:
                signal = self._cross_capture_signal
                self._cross_capture_signal = None 
                
                if face_frame is not None and plate_frame is not None:
                    try:
                        # (A) æº–å‚™æª”åè³‡è¨Š
                        now = datetime.now()
                        readable_ts = now.strftime("%mæœˆ%dæ—¥_%Hé»%Måˆ†%Sç§’")
                        e1_name, e1_score = signal["top1"]
                        e2_name, e2_score = signal["top2"]
                        emo_tag_1 = f"{e1_name}-{int(e1_score)}"
                        emo_tag_2 = f"{e2_name}-{int(e2_score)}"
                        
                        # (B) è™•ç†äººè‡‰ (Face) - ä¿æŒç°¡å–®ï¼Œç›´æ¥å­˜
                        face_filename = f"{readable_ts}_{emo_tag_1}_{emo_tag_2}_Face.jpg"
                        face_path = self._save_custom_file(face_filename, face_frame)
                        if face_path:
                            self.db_manager.save_event_evidence(
                                self.session_id, "strong_emotion_face", face_path
                            )
                        
                        # (C) è™•ç†é¤ç›¤ (Plate) - â˜… [ä¿®æ”¹] å•Ÿå‹•èƒŒæ™¯è¾¨è­˜
                        plate_filename = f"{readable_ts}_{emo_tag_1}_{emo_tag_2}_Plate.jpg"
                        plate_path = self._save_custom_file(plate_filename, plate_frame)
                        
                        if plate_path:
                            print(f"ğŸ“¸ é›™é¡é ­å¿«ç…§å®Œæˆï¼Œæ­£åœ¨èƒŒæ™¯è¾¨è­˜é£Ÿç‰©...")
                            # å•Ÿå‹•ä¸€æ¢æ–°åŸ·è¡Œç·’å»è·‘ LLMï¼Œé¿å…å¡ä½ä¸»ç•«é¢
                            threading.Thread(
                                target=self._background_identify_and_save,
                                args=(plate_frame.copy(), plate_path, self.session_id)
                            ).start()
                            
                    except Exception as e:
                        print(f"Snapshot Error: {e}")
            # =========================================================

            # 3. è™•ç†ç•°æ­¥å›å‚³çš„è³‡æ–™
            if self._cached_plate_insight: 
                result.plate_insight = self._cached_plate_insight
                self._cached_plate_insight = None
                
            if self._cached_token_usage:
                result.token_usage_event = self._cached_token_usage
                self._cached_token_usage = None 

            # 4. åŒæ­¥å„²å­˜ Log
            self._sync_log_task()
            
            # 5. æ¨é€çµæœ
            if self._analysis_result_queue.full():
                try: self._analysis_result_queue.get_nowait()
                except Empty: pass
            self._analysis_result_queue.put_nowait(result)
            
            time.sleep(0.005)

    def _run_vlm_background(self, frame, client, food_detections):
        try:
            async def task():
                insight, usage = await llm.analyze_plate_vlm(frame, client) 
                return insight, usage
            insight, usage = asyncio.run(task())
            
            if insight:
                self._cached_plate_insight = insight 
                self._new_insight_arrived = True 
            if usage:
                current = self._cached_token_usage or {"total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}
                current["total_tokens"] += usage.total_tokens
                current["prompt_tokens"] += usage.prompt_tokens
                current["completion_tokens"] += usage.completion_tokens
                self._cached_token_usage = current
        except Exception as e:
            print(f"VLM Error: {e}")
        finally:
            self._vlm_busy = False 

    def _background_identify_and_save(self, frame, local_path, session_id):
        """
        [NEW] èƒŒæ™¯ä»»å‹™ï¼šè¾¨è­˜é£Ÿç‰©ä¸¦å­˜å…¥ DB
        """
        try:
            client = self.model_pack.get("client")
            # å®šç¾©å€™é¸èœå–® (æ‚¨å¯ä»¥å¾ config æˆ–å¤–éƒ¨å‚³å…¥)
            menu_list = self.menu_items if self.menu_items else ["æ¼¢å ¡", "é›å¡Š", "è–¯æ¢"]
            
            # 1. å‘¼å« LLM è¾¨è­˜ (åŒæ­¥åŸ·è¡Œ async å‡½å¼)
            # å› ç‚ºé€™æ˜¯åœ¨ç¨ç«‹çš„ Thread è·‘ï¼Œæ‰€ä»¥ç”¨ asyncio.run æ˜¯å®‰å…¨çš„
            food_name = asyncio.run(identify_food_item(frame, menu_list, client))
            
            print(f"ğŸ” [AI è¾¨è­˜çµæœ] {food_name}")

            # 2. å­˜å…¥è³‡æ–™åº« (å¸¶æœ‰ food_label)
            self.db_manager.save_event_evidence(
                session_id=session_id, 
                event_type="strong_emotion_plate", 
                local_path=local_path,
                food_label=food_name  # â˜… æŠŠè¾¨è­˜çµæœå­˜é€²å»
            )
            
        except Exception as e:
            print(f"Food ID Task Error: {e}")
            # å¤±æ•—ä¹Ÿè¦å­˜ï¼Œä½† label æ˜¯ None
            self.db_manager.save_event_evidence(session_id, "strong_emotion_plate", local_path, "Unknown")

    def _run_deepface_background(self, frame, face_detector):
        try:
            face_crop = crop_face_with_mediapipe(frame, face_detector)
            if face_crop is None: return

            # 1. åŸ·è¡Œåˆ†æ
            analysis = DeepFace.analyze(
                img_path=face_crop, 
                actions=['emotion'], 
                enforce_detection=False, 
                detector_backend='skip', 
                silent=True
            )
            
            result = analysis[0]
            emotions_dict = result['emotion'] # å–å¾—æ‰€æœ‰æƒ…ç·’çš„åˆ†æ•¸å­—å…¸
            
            # æ’åºï¼šç”±é«˜åˆ°ä½ [(emotion, score), ...]
            sorted_emotions = sorted(emotions_dict.items(), key=lambda item: item[1], reverse=True)
            
            # ç¬¬ä¸€å
            top1_name = sorted_emotions[0][0]
            top1_score = sorted_emotions[0][1]
            
            # ç¬¬äºŒå (ä»¥é˜²è¬ä¸€åªæœ‰ä¸€å€‹ï¼Œåšå€‹æª¢æŸ¥)
            top2_name = sorted_emotions[1][0] if len(sorted_emotions) > 1 else "neutral"
            top2_score = sorted_emotions[1][1] if len(sorted_emotions) > 1 else 0.0

            # 3. ä¸­æ–‡æ˜ å°„ (Mapping)
            mapping = {
                "happy": "é–‹å¿ƒ", "neutral": "å¹³æ·¡", "sad": "å¤±æœ›", 
                "angry": "ä¸æ»¿", "surprise": "é©šè‰·", "fear": "å›°æƒ‘", "disgust": "å«Œæ£„"
            }
            top1_zh = mapping.get(top1_name, top1_name)
            top2_zh = mapping.get(top2_name, top2_name)

            # â˜…â˜…â˜… [é‡é» 1] æ›´æ–°å¿«å– (çµ¦ DB ç”¨)ï¼šä¿æŒç´”æ–‡å­— â˜…â˜…â˜…
            self._cached_emotion = top1_zh 
            
            # 4. å¼·çƒˆæƒ…ç·’è§¸ç™¼é‚è¼¯
            # æ¢ä»¶ï¼šç¬¬ä¸€åä¸æ˜¯å¹³æ·¡ï¼Œä¸”åˆ†æ•¸ > 40% (æ‚¨è¨­å®šçš„å€¼)
            INTENSITY_THRESHOLD = 40.0 
            
            if top1_name != "neutral" and top1_score > INTENSITY_THRESHOLD:
                print(f"ğŸ”¥ å¼·çƒˆæƒ…ç·’: {top1_zh}({top1_score:.0f}%) / {top2_zh}({top2_score:.0f}%)")
                
                # ç™¼é€è¨Šè™Ÿï¼šå‚³éæ›´å®Œæ•´çš„è³‡è¨Š
                self._cross_capture_signal = {
                    "top1": (top1_zh, top1_score), 
                    "top2": (top2_zh, top2_score) 
                }

            # â˜…â˜…â˜… [é‡é» 2] Log é–å®š (çµ¦ UI ç”¨)ï¼šåŠ ä¸Šåˆ†æ•¸ â˜…â˜…â˜…
            with self._latch_lock:
                # é€™è£¡æ”¹æˆ formatted stringï¼Œè®“ä»‹é¢é¡¯ç¤º "é–‹å¿ƒ (98%)"
                self._latched_emotion = f"{top1_zh} ({top1_score:.0f}%)"
                
        except Exception as e:
            print(f"DeepFace Error: {e}")
        finally:
            self._llm_busy = False

    def start(self):
        if self._face_cam_thread and self._face_cam_thread.is_alive(): return
        self._stop_event.clear()
        
        # [ä¿®æ”¹] å•Ÿå‹•å…©å€‹ç›¸æ©ŸåŸ·è¡Œç·’ + ä¸€å€‹åˆ†æåŸ·è¡Œç·’
        self._face_cam_thread = threading.Thread(target=self._face_cam_loop, daemon=True)
        self._plate_cam_thread = threading.Thread(target=self._plate_cam_loop, daemon=True)
        self._worker_thread = threading.Thread(target=self._analysis_worker, daemon=True)
        
        self._face_cam_thread.start()
        self._plate_cam_thread.start()
        self._worker_thread.start()

    def stop(self):
        self._stop_event.set()
        time.sleep(0.5)
        self._face_cam_thread = None
        self._plate_cam_thread = None
        self._worker_thread = None

    # [ä¿®æ”¹] å›å‚³å…©å¼µåœ– (Face, Plate)
    def get_latest_frames(self):
        f_frame = None
        p_frame = None
        try: f_frame = self._face_display_queue.get_nowait()
        except Empty: pass
        try: p_frame = self._plate_display_queue.get_nowait()
        except Empty: pass
        return f_frame, p_frame

    @property
    def raw_session_id(self): return self.session_id

    def get_latest_analysis_result(self):
        try:
            result = self._analysis_result_queue.get_nowait()
        except Empty:
            return None

        with self._latch_lock:
            if self._latched_nod:
                result.nod_event = True
                self._latched_nod = False
            if self._latched_shake:
                result.shake_event = True 
                self._latched_shake = False
            
            # å–å‡ºæƒ…ç·’äº‹ä»¶
            if self._latched_emotion:
                result.emotion_event = self._latched_emotion
                self._latched_emotion = None 

        return result