# services/llm_handler.py

import base64
import io
import json
import asyncio
import httpx
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion
from PIL import Image
import cv2
from typing import List, Dict, Tuple, Any

# --- å°å…¥æˆ‘å€‘å»ºç«‹çš„ prompt è®€å–å·¥å…· ---
# è«‹ç¢ºä¿ utils/prompt_loader.py å­˜åœ¨ï¼Œè‹¥ç„¡å¯æš«æ™‚è¨»è§£ä¸¦å°‡ Prompt å¯«æ­»åœ¨å‡½å¼å…§
try:
    from utils.prompt_loader import load_prompt_template
except ImportError:
    # ç°¡å–®çš„ fallbackï¼Œé¿å…å¦‚æœæ²’æœ‰é€™å€‹æª”æ¡ˆæ™‚å ±éŒ¯
    def load_prompt_template(name, type):
        return "" 

# ä½¿ç”¨ AsyncClient é€²è¡ŒéåŒæ­¥è«‹æ±‚
aclient = httpx.AsyncClient()

# åœ–ç‰‡æœ€å¤§é‚Šé•·é™åˆ¶ (åŠ é€Ÿ VLM åˆ†æç”¨)
MAX_VLM_IMAGE_DIM = 768

def get_openai_client(api_key):
    """æ ¹æ“š API Key åˆå§‹åŒ–ä¸¦è¿”å›ç•°æ­¥çš„ OpenAI å®¢æˆ¶ç«¯ç‰©ä»¶ã€‚"""
    if not api_key:
        return None
    try:
        return AsyncOpenAI(api_key=api_key)
    except Exception as e:
        print(f"åˆå§‹åŒ– OpenAI Client å¤±æ•—: {e}")
        return None

def _image_to_base64(pil_image: Image.Image) -> str:
    """å°‡ PIL.Image ç‰©ä»¶è½‰æ›ç‚º base64 å­—ä¸²ï¼Œä¸¦è‡ªå‹•ç¸®æ”¾ä»¥åŠ é€Ÿ VLM åˆ†æã€‚"""
    width, height = pil_image.size
    
    if max(width, height) > MAX_VLM_IMAGE_DIM:
        if width > height:
            new_width = MAX_VLM_IMAGE_DIM
            new_height = int(height * (new_width / width))
        else:
            new_height = MAX_VLM_IMAGE_DIM
            new_width = int(width * (new_height / height))
        
        pil_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)

    buffered = io.BytesIO()
    pil_image.save(buffered, format="JPEG", quality=85)
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

# ==========================================
# 1. å½±åƒè¾¨è­˜ç›¸é—œ (Vision / VLM)
# ==========================================

async def gpt_image_classify_3cls(face_bgr, client: AsyncOpenAI, model="gpt-4o-mini"):
    """
    (éåŒæ­¥) ä½¿ç”¨ GPT-4o-mini é€²è¡Œä¸‰åˆ†é¡è¡¨æƒ…è¾¨è­˜ã€‚
    è¼¸å…¥: OpenCV BGR å½±åƒ
    è¼¸å‡º: (æƒ…ç·’å­—ä¸², usage_data)
    """
    if face_bgr is None: return "ç„¡è‡‰", None
    if client is None: return "ï¼ˆæœªè¨­å®š APIï¼‰", None

    pil_img = Image.fromarray(cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB))
    b64_img = _image_to_base64(pil_img)
    
    prompt = (
        "è«‹æ ¹æ“šè‡‰éƒ¨è¡¨æƒ…ï¼Œåœ¨ä¸‰é¡ä¸­æ“‡ä¸€è¼¸å‡ºï¼ˆè«‹åªè¼¸å‡ºä¸€å€‹è©ï¼‰ï¼š\n"
        "ã€å–œæ­¡ã€ï¼ˆæ­£å‘/å¾®ç¬‘ï¼‰ã€ã€ä¸­æ€§ã€ã€æˆ–ã€è¨å­ã€ï¼ˆå­æƒ¡/çšºçœ‰ï¼‰ã€‚\n"
        "åªè¼¸å‡ºï¼šå–œæ­¡ / ä¸­æ€§ /è¨å­ã€‚"
    )
    
    try:
        resp = await client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}},
                ],
            }],
            temperature=0, max_tokens=10
        )
        text = resp.choices[0].message.content.strip()
        usage = resp.usage

        emotion = "ä¸­æ€§"
        if "å–œæ­¡" in text: emotion = "å–œæ­¡"
        if "è¨å­" in text: emotion = "è¨å­"
        
        return emotion, usage

    except Exception as e:
        print(f"è¡¨æƒ…åˆ†é¡ API éŒ¯èª¤: {e}")
        return "API éŒ¯èª¤", None


async def identify_food_item(plate_bgr, menu_items: list, client: AsyncOpenAI, model="gpt-4o"):
    """
    é‡å°å¼·çƒˆæƒ…ç·’è§¸ç™¼çš„å¿«ç…§ï¼Œé€²è¡Œé«˜ç²¾æº–åº¦çš„é£Ÿç‰©è¾¨è­˜ã€‚
    
    Args:
        plate_bgr: OpenCV æ ¼å¼çš„å½±åƒ (BGR)
        menu_items: å€™é¸èœå–®åˆ—è¡¨ (ä¾‹å¦‚ ["æ¼¢å ¡", "é›å¡Š", "è–¯æ¢"])
        client: OpenAI Client
        model: å»ºè­°ä½¿ç”¨ gpt-4o ä»¥ç²å¾—æœ€ä½³è¦–è¦ºè¾¨è­˜èƒ½åŠ›
        
    Returns:
        str: è¾¨è­˜å‡ºçš„é£Ÿç‰©åç¨± (ä¾‹å¦‚ "æ¼¢å ¡")ï¼Œè‹¥å¤±æ•—å‰‡å›å‚³ "Unknown"
    """
    if plate_bgr is None: return "Unknown"
    if client is None: return "No_API_Key"

    # 1. åœ–ç‰‡è½‰ Base64
    pil_img = Image.fromarray(cv2.cvtColor(plate_bgr, cv2.COLOR_BGR2RGB))
    b64_img = _image_to_base64(pil_img)

    # 2. æº–å‚™é¸å–®å­—ä¸² (ç¢ºä¿æœ‰ Other é¸é …)
    safe_menu = menu_items.copy() if menu_items else []
    if "å…¶ä»–" not in safe_menu and "Other" not in safe_menu:
        safe_menu.append("Other")
    
    options_str = ", ".join(safe_menu)
    
    # 3. å»ºæ§‹ Prompt (è¦æ±‚ç²¾ç°¡å›ç­”)
    prompt = (
        f"è«‹è§€å¯Ÿé€™å¼µé¤ç›¤ç…§ç‰‡ï¼Œä¸¦å¾ä»¥ä¸‹æ¸…å–®ä¸­é¸å‡ºæœ€ç¬¦åˆçš„é£Ÿç‰©åç¨±ï¼š\n"
        f"æ¸…å–®ï¼š[{options_str}]\n"
        "è«‹ç›´æ¥å›å‚³è©²åç¨±ï¼Œä¸è¦åŠ ä»»ä½•æ¨™é»ç¬¦è™Ÿæˆ–é¡å¤–æ–‡å­—(ä¾‹å¦‚ä¸è¦å›å‚³ 'æ˜¯æ¼¢å ¡'ï¼Œåªè¦å›å‚³ 'æ¼¢å ¡')ã€‚\n"
        "å¦‚æœå®Œå…¨çœ‹ä¸å‡ºä¾†ã€ç©ºç›¤æˆ–ä¸åœ¨æ¸…å–®ä¸­ï¼Œè«‹å›å‚³ 'Other'ã€‚"
    )

    try:
        resp = await client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}},
                ],
            }],
            temperature=0.1, # ä½éš¨æ©Ÿæ€§ï¼Œç¢ºä¿ç­”æ¡ˆç©©å®š
            max_tokens=20
        )
        
        # 4. å–å¾—çµæœ
        text = resp.choices[0].message.content.strip()
        
        # ç°¡å–®é˜²å‘†ï¼šç¢ºä¿å›å‚³çš„æ–‡å­—çœŸçš„åœ¨æˆ‘å€‘çš„æ¸…å–®è£¡
        # (æœ‰æ™‚å€™ LLM æœƒå›å‚³ "æ‡‰è©²æ˜¯æ¼¢å ¡"ï¼Œæˆ‘å€‘åªè¦ "æ¼¢å ¡")
        for item in safe_menu:
            if item in text:
                return item
                
        return text # å¦‚æœéƒ½æ²’å°åˆ°ï¼Œå°±å›å‚³åŸå§‹ç­”æ¡ˆ (é€šå¸¸æ˜¯ Other)

    except Exception as e:
        print(f"Food ID Error: {e}")
        return "Error"


async def analyze_plate_vlm(plate_bgr: Any, client: AsyncOpenAI, 
                            food_detections: List[Dict[str, Any]] = None, 
                            model="gpt-4o-mini"):
    """
    (éåŒæ­¥) ä½¿ç”¨ VLM (GPT-4o-mini) åˆ†æé¤ç›¤å‰©é£ŸåŸå› ã€‚
    """
    if plate_bgr is None: return None, None
    if client is None: return None, None

    pil_img = Image.fromarray(cv2.cvtColor(plate_bgr, cv2.COLOR_BGR2RGB))
    b64_img = _image_to_base64(pil_img)

    yolo_info = ""
    if food_detections:
        formatted_dets = [
            f"{det['label']} (ä¿¡å¿ƒåº¦: {det['conf']:.2f})"
            for det in food_detections
        ]
        if formatted_dets:
            yolo_info = "\n[ç³»çµ±æç¤º] YOLO æ¼”ç®—æ³•åœ¨ç•«é¢ä¸­åµæ¸¬åˆ°äº†ï¼š " + ", ".join(formatted_dets)

    system_prompt = (
        "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é¤å»³ç‡Ÿé‹é¡§å•ã€‚è«‹è§€å¯Ÿé€™å¼µé¤ç›¤å›æ”¶çš„ç…§ç‰‡ã€‚"
        "è«‹å…ˆåˆ¤æ–·é¤ç›¤ç‹€æ…‹ï¼Œä¸¦ä¾æ“šæƒ…æ³æ“‡ä¸€å›ç­” (ç¹é«”ä¸­æ–‡ï¼Œ50å­—ä»¥å…§)ï¼š\n\n"
        "æƒ…æ³ Aï¼šå¦‚æœæ˜¯ã€ç©ºç›¤ã€æˆ–ã€åƒå¾—å¾ˆä¹¾æ·¨ã€\n"
        "è«‹å›ç­”ï¼šã€Œé¡§å®¢å°é¤é»æ¥å—åº¦é«˜ï¼Œå®Œé£Ÿç„¡æµªè²»ã€‚ã€é€™é¡æ­£é¢è©•åƒ¹ï¼Œä¸è¦æé€ å‰©é£ŸåŸå› ã€‚\n\n"
        "æƒ…æ³ Bï¼šå¦‚æœæœ‰ã€æ˜é¡¯å‰©é£Ÿã€\n"
        "è«‹å›ç­”ï¼š\n"
        "1. å‰©ä¸‹äº†ä»€éº¼å…·é«”é£Ÿç‰©ï¼Ÿ(ä¾‹å¦‚ï¼šé’æ¤’ã€æ¾±ç²‰ã€è‚‰é¡)\n"
        "2. æ¨æ¸¬å‰©é£ŸåŸå› ï¼Ÿ(ä¾‹å¦‚ï¼šå®Œå…¨æœªå‹•å¯èƒ½æ˜¯ä¸æ„›åƒã€å‰©ä¸€åŠå¯èƒ½æ˜¯ä»½é‡å¤ªå¤§)"
    )
    
    user_prompt = f"è«‹åˆ†æé€™å¼µå‰©é£Ÿç…§ç‰‡ã€‚{yolo_info}"

    try:
        resp = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}},
                    ],
                }
            ],
            max_tokens=150, 
            temperature=0.3 
        )
        content = resp.choices[0].message.content.strip()
        usage = resp.usage
        return content, usage
    except Exception as e:
        print(f"VLM é¤ç›¤åˆ†æéŒ¯èª¤: {e}")
        return f"API éŒ¯èª¤: {str(e)[:50]}", None


# ==========================================
# 2. æ–‡æœ¬å ±å‘Šèˆ‡æ‘˜è¦ç›¸é—œ (Report Generation)
# ==========================================

async def summarize_session(stats: dict, 
                            client: AsyncOpenAI, 
                            store_type: str = "é¤å»³", 
                            tone: str = "å°ˆæ¥­", 
                            tips_style: str = "é è¨­",
                            custom_instructions: str = None, 
                            model="gpt-4o-mini"):
    """
    (éåŒæ­¥) æ ¹æ“šçµ±è¨ˆæ•¸æ“šï¼Œç”Ÿæˆå®¢è£½åŒ–çš„é¡§å®¢é«”é©—æ‘˜è¦å ±å‘Šã€‚
    æ”¯æ´ custom_instructions åƒæ•¸ï¼Œè‹¥æœ‰å‚³å…¥å‰‡ç›´æ¥ä½¿ç”¨è©²æŒ‡ä»¤ (ç”¨æ–¼ Dashboard ç‡Ÿé‹å ±å‘Š)ã€‚
    """
    if client is None:
        return "ï¼ˆæœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•ç”¢ç”Ÿæ‘˜è¦ï¼‰", None

    # 1. å˜—è©¦è®€å– Prompt æ¨¡æ¿ï¼Œè‹¥å¤±æ•—å‰‡ä½¿ç”¨é è¨­å€¼
    try:
        system_template = load_prompt_template('summarize_session', 'system')
        if not system_template: raise FileNotFoundError
    except:
        system_template = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é¤å»³ç‡Ÿé‹é¡§å•ã€‚è«‹æ ¹æ“šæ•¸æ“šç”Ÿæˆå ±å‘Šã€‚"
        
    try:
        user_template = load_prompt_template('summarize_session', 'user')
        if not user_template: raise FileNotFoundError
    except:
        user_template = "æ•¸æ“šå¦‚ä¸‹ï¼š{stats_json}ã€‚è«‹åˆ†æã€‚"

    # 2. è™•ç† System Prompt
    system_prompt = system_template.replace('{store_type}', str(store_type)) \
                                   .replace('{tone}', str(tone)) \
                                   .replace('{tips_style}', str(tips_style))

    # 3. è™•ç† User Prompt
    if custom_instructions:
        # â˜… å¦‚æœæœ‰è‡ªè¨‚æŒ‡ä»¤ (Dashboard Tab 1)ï¼Œç›´æ¥ä½¿ç”¨å®ƒ
        user_prompt = custom_instructions
    else:
        # â˜… å¦å‰‡ä½¿ç”¨é è¨­æ¨¡æ¿ä¸¦å¡«å…¥æ•¸æ“š (Video/Live æ¨¡å¼)
        json_str = json.dumps(stats, indent=2, ensure_ascii=False)
        user_prompt = user_template.replace('{stats_json}', json_str)
    
    try:
        r: ChatCompletion = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7, max_tokens=800,
        )
        summary_text = r.choices[0].message.content
        usage_data = r.usage
        return summary_text, usage_data
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
        print(f"æ‘˜è¦ç”Ÿæˆ API éŒ¯èª¤: {e}")
        return error_msg, None


async def generate_menu_report(food_stats: dict, client: AsyncOpenAI, model="gpt-4o-mini"):
    """
    (éåŒæ­¥) å°ˆé–€é‡å°ã€Œèœè‰²ã€ç”Ÿæˆç ”ç™¼èˆ‡èª¿æ•´å»ºè­°å ±å‘Š (Tab 2 å°ˆç”¨)ã€‚
    """
    if not client: return "ï¼ˆæœªè¨­å®š API Keyï¼‰", None

    # å°‡çµ±è¨ˆæ•¸æ“šè½‰ç‚º JSON å­—ä¸²
    stats_str = json.dumps(food_stats, ensure_ascii=False, indent=2)

    system_prompt = (
        "ä½ æ˜¯ä¸€ä½æ“æœ‰ 20 å¹´ç¶“é©—çš„ã€é¤é£²èœè‰²ç ”ç™¼é¡§å•ã€ã€‚ä½ çš„å·¥ä½œæ˜¯æ ¹æ“šé¡§å®¢å°ç‰¹å®šèœè‰²çš„æƒ…ç·’åæ‡‰æ•¸æ“šï¼Œ"
        "ç‚ºå…§å ´ä¸»å»šæä¾›å…·é«”çš„èœå–®èª¿æ•´å»ºè­°ã€‚\n\n"
        "ã€æ•¸æ“šèªªæ˜ã€‘\n"
        "è¼¸å…¥çš„ JSON æ ¼å¼ç‚ºï¼š{'èœå': {'é–‹å¿ƒ': æ¬¡æ•¸, 'å«Œæ£„': æ¬¡æ•¸, ...}}\n\n"
        "ã€å ±å‘Šçµæ§‹è¦æ±‚ã€‘\n"
        "1. ğŸ† **æ˜æ˜Ÿèœè‰² (Star Dishes)**ï¼šæ­£é¢æƒ…ç·’ä½”æ¯”æœ€é«˜çš„èœã€‚åˆ†æå…¶æˆåŠŸå¯èƒ½çš„å› ç´ ï¼ˆå£å‘³ã€è³£ç›¸ï¼‰ã€‚\n"
        "2. ğŸ’£ **å•é¡Œèœè‰² (Problem Dishes)**ï¼šè² é¢æƒ…ç·’ï¼ˆå«Œæ£„/å¤±æœ›/ä¸æ»¿ï¼‰è¼ƒé«˜çš„èœã€‚è«‹å¤§è†½æ¨æ¸¬å¯èƒ½åŸå› ï¼ˆå¦‚ï¼šèª¿å‘³éé‡ã€å†·æ‰ã€é£Ÿææ­é…æ€ªç•°ï¼‰ã€‚\n"
        "3. ğŸ”ª **ä¸»å»šè¡Œå‹•å»ºè­° (Action Plan)**ï¼šé‡å°å•é¡Œèœè‰²ï¼Œæå‡º 2-3 å€‹å…·é«”çš„æ”¹è‰¯æ–¹å‘ï¼ˆä¾‹å¦‚ï¼šèª¿æ•´é†¬æ±æ¯”ä¾‹ã€æ›´æ›ç››ç›¤æ–¹å¼ï¼‰ã€‚\n"
        "è«‹ç”¨å°ˆæ¥­ã€ç›´ç™½ä¸”å»ºè¨­æ€§çš„èªæ°£æ’°å¯«ï¼Œä¸è¦è¬›ç©ºè©±ã€‚"
    )

    user_prompt = f"è«‹åˆ†ææœ¬é€±çš„èœè‰²æƒ…ç·’æ•¸æ“šï¼š\n{stats_str}"

    try:
        resp = await client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        return resp.choices[0].message.content, resp.usage
    except Exception as e:
        return f"ç”Ÿæˆèœè‰²å ±å‘Šå¤±æ•—: {e}", None

# ==========================================
# 3. åŒæ­¥è¼”åŠ©å‡½å¼ (Sync Helper for ThreadPool)
# ==========================================

def sync_gpt_image_classify_3cls(face_img, client):
    """
    åŒæ­¥ç‰ˆæœ¬çš„ GPT-4o åœ–ç‰‡æƒ…ç·’è¾¨è­˜ (å°ˆä¾› ThreadPoolExecutor ä½¿ç”¨)
    è¼¸å…¥: OpenCV å½±åƒ (BGR)
    è¼¸å‡º: 'å–œæ­¡', 'ä¸­æ€§', 'è¨å­' å…¶ä¸­ä¹‹ä¸€
    """
    if face_img is None or face_img.size == 0:
        return "ä¸­æ€§"

    try:
        # 1. å½±åƒç·¨ç¢¼ (BGR -> JPEG -> Base64)
        _, buffer = cv2.imencode('.jpg', face_img)
        img_base64 = base64.b64encode(buffer).decode('utf-8')

        # 2. å‘¼å« GPT-4o (åŒæ­¥æ¨¡å¼ï¼Œä¸åŠ  await)
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system", 
                    "content": "You are an emotion classifier. Classify the face image into exactly one of these 3 classes: 'å–œæ­¡', 'ä¸­æ€§', 'è¨å­'. Return ONLY the class name."
                },
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": "Classify this face."},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}}
                    ]
                }
            ],
            max_tokens=10,
            temperature=0.3
        )
        
        result = response.choices[0].message.content.strip()
        
        # ç°¡å–®é˜²å‘†æ¸…æ´—
        for valid in ["å–œæ­¡", "ä¸­æ€§", "è¨å­"]:
            if valid in result:
                return valid
                
        return "ä¸­æ€§" # é è¨­å€¼

    except Exception as e:
        print(f"LLM Sync Error: {e}")
        return "ä¸­æ€§"